{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pl_core_news_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uau-U0b7r19E",
        "outputId": "46c97aad-ee48-4dda-9047-17b86fa9eae4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pl-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_lg-3.7.0/pl_core_news_lg-3.7.0-py3-none-any.whl (573.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m573.7/573.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pl-core-news-lg==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pl-core-news-lg==3.7.0) (0.1.2)\n",
            "Installing collected packages: pl-core-news-lg\n",
            "Successfully installed pl-core-news-lg-3.7.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pl_core_news_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('pl_core_news_lg')\n"
      ],
      "metadata": {
        "id": "2iPcmUYcDbbx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "    \"\"\"\n",
        "    Part 1 - Processing and cleaning the data:\n",
        "    - Convert weekdays to full names\n",
        "    - Convert months to numbers\n",
        "    - Extract links\n",
        "    - Remove stopwords\n",
        "    - Add a clean date column in DD.MM.YYYY format\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Task 1.1: Replace weekday abbreviations\n",
        "    weekday_map = {\n",
        "        'Mon': 'Monday', 'Tue': 'Tuesday', 'Wed': 'Wednesday',\n",
        "        'Thu': 'Thursday', 'Fri': 'Friday', 'Sat': 'Saturday', 'Sun': 'Sunday'\n",
        "    }\n",
        "    df['weekday'] = df['created_at'].str.split().str[0].map(weekday_map)\n",
        "\n",
        "    # Task 1.2: Replace month abbreviations with numbers\n",
        "    month_map = {\n",
        "        'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05',\n",
        "        'Jun': '06', 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10',\n",
        "        'Nov': '11', 'Dec': '12'\n",
        "    }\n",
        "\n",
        "    def convert_date(date_str):\n",
        "        if pd.isna(date_str):\n",
        "            return date_str\n",
        "        parts = date_str.split()\n",
        "        if len(parts) >= 2:\n",
        "            parts[1] = month_map.get(parts[1], parts[1])\n",
        "        return ' '.join(parts)\n",
        "\n",
        "    df['user_created_at_converted'] = df['user_created_at'].apply(convert_date)\n",
        "\n",
        "    # Add a clean date column in DD.MM.YYYY format\n",
        "    df['clean_date'] = pd.to_datetime(df['user_created_at'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
        "    df['clean_date'] = df['clean_date'].dt.strftime('%d.%m.%Y')\n",
        "\n",
        "    # Tasks 1.3, 1.4, 1.5: Extract links to lists\n",
        "    tweet_links = df['tweet_url'].dropna().tolist()\n",
        "    url_links = df['urls'].dropna().tolist()\n",
        "    media_links = df['media'].dropna().tolist()\n",
        "\n",
        "    # Task 1.6: Remove stopwords\n",
        "    def remove_stopwords(text):\n",
        "        if pd.isna(text):\n",
        "            return ''\n",
        "        doc = nlp(str(text))\n",
        "        return ' '.join([token.text for token in doc if not token.is_stop])\n",
        "\n",
        "    df['text_without_stopwords'] = df['text'].apply(remove_stopwords)\n",
        "\n",
        "    # Save links to files\n",
        "    with open('tweet_links.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(tweet_links))\n",
        "    with open('url_links.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(url_links))\n",
        "    with open('media_links.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(media_links))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "U46hKUXHDhKd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analyze_data(df):\n",
        "    \"\"\"\n",
        "    Part 2 - Exploratory data analysis:\n",
        "    Generate all required analysis results\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        # Task 2.1: Top 5 by likes\n",
        "        'top_likes': df.nlargest(5, 'favorite_count')[['text', 'favorite_count']],\n",
        "        # Task 2.2: Top 5 by retweets\n",
        "        'top_retweets': df.nlargest(5, 'retweet_count')[['text', 'retweet_count']],\n",
        "        # Task 2.3: Non-sensitive tweets\n",
        "        'non_sensitive': df[df['possibly_sensitive'] == 'FALSE'],\n",
        "        # Task 2.4: Earliest account tweets\n",
        "        'earliest_user_tweets': df[df['user_created_at'] == df['user_created_at'].min()],\n",
        "        # Task 2.5: Most followed user tweets\n",
        "        'most_followed_tweets': df[df['user_followers_count'] == df['user_followers_count'].max()],\n",
        "        # Task 2.6: Verified users\n",
        "        'verified_users': df[df['user_verified'] == True],\n",
        "        # Task 2.7: Most common day\n",
        "        'most_common_day': df['created_at'].str.split().str[0].mode()[0]\n",
        "    }\n",
        "\n",
        "    # Save analysis results\n",
        "    pd.DataFrame(results['top_likes']).to_csv('top_likes.csv', index=False)\n",
        "    pd.DataFrame(results['top_retweets']).to_csv('top_retweets.csv', index=False)\n",
        "    pd.DataFrame(results['earliest_user_tweets']).to_csv('earliest_user_tweets.csv', index=False)\n",
        "    pd.DataFrame(results['most_followed_tweets']).to_csv('most_followed_tweets.csv', index=False)\n",
        "    pd.DataFrame(results['verified_users']).to_csv('verified_users.csv', index=False)\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "WTm2tAU2DoJT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_nlp(df):\n",
        "    \"\"\"\n",
        "    Part 3 - Natural language processing:\n",
        "    Extract entities and create required columns\n",
        "    \"\"\"\n",
        "    # Load Polish model\n",
        "    nlp = spacy.load('pl_core_news_lg')\n",
        "\n",
        "    def extract_entities(text):\n",
        "        doc = nlp(str(text))\n",
        "\n",
        "        # Task 3.1: Extract persons\n",
        "        persons = []\n",
        "        # Task 3.2: Extract places\n",
        "        places = []\n",
        "        # Task 3.3: Extract organizations\n",
        "        orgs = []\n",
        "\n",
        "        # First pass - using spaCy's NER\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in ['persName', 'PERSON']:\n",
        "                persons.append(ent.text)\n",
        "            elif ent.label_ in ['placeName', 'GPE', 'LOC']:\n",
        "                places.append(ent.text)\n",
        "            elif ent.label_ in ['orgName', 'ORG']:\n",
        "                orgs.append(ent.text)\n",
        "\n",
        "        # Second pass - custom rules for organizations\n",
        "        words = text.split()\n",
        "        for i, word in enumerate(words):\n",
        "            if word[0].isupper() and i < len(words)-1:\n",
        "                phrase = word\n",
        "                next_word = words[i+1]\n",
        "                if next_word[0].isupper() or next_word.lower() in ['m.st.', 'sp.', 'z.o.o', 'sa']:\n",
        "                    phrase += ' ' + next_word\n",
        "                    if any(org_word in phrase for org_word in ['Stra≈º', 'UrzƒÖd', 'Komitet', 'Fundacja']):\n",
        "                        orgs.append(phrase)\n",
        "\n",
        "        return persons, places, orgs\n",
        "\n",
        "    # Create new columns for entities\n",
        "    df[['persons', 'places', 'organisations']] = pd.DataFrame(\n",
        "        df['text'].apply(extract_entities).tolist(),\n",
        "        index=df.index\n",
        "    )\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "V5g9EQRjDqKU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_weekday_plot(df):\n",
        "    \"\"\"\n",
        "    Part 4 - Visualization:\n",
        "    Create matplotlib graph of tweets per weekday\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    weekday_counts = df['created_at'].str.split().str[0].value_counts()\n",
        "    weekday_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "    weekday_counts = weekday_counts.reindex(weekday_order)\n",
        "\n",
        "    plt.bar(weekday_counts.index, weekday_counts.values)\n",
        "    plt.title('Number of Tweets per Day of the Week')\n",
        "    plt.xlabel('Day of the Week')\n",
        "    plt.ylabel('Number of Tweets')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    plt.savefig('tweets_per_day.png')\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "ulDn3IVdDrr2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "print(\"Starting data analysis...\")\n",
        "\n",
        "# Read data\n",
        "df = pd.read_csv('dane1.csv')\n",
        "\n",
        "# Part 1: Clean data\n",
        "print(\"\\nPart 1: Cleaning data...\")\n",
        "cleaned_df = clean_data(df)\n",
        "\n",
        "# Part 2: Analysis\n",
        "print(\"\\nPart 2: Performing analysis...\")\n",
        "analysis_results = analyze_data(cleaned_df)\n",
        "\n",
        "# Part 3: NLP\n",
        "print(\"\\nPart 3: Processing text with NLP...\")\n",
        "processed_df = process_nlp(cleaned_df)\n",
        "\n",
        "# Save final processed data with all new columns\n",
        "processed_df.to_csv('processed_tweets.csv', index=False)\n",
        "\n",
        "# Part 4: Visualization\n",
        "print(\"\\nPart 4: Creating visualization...\")\n",
        "create_weekday_plot(processed_df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nResults preview:\")\n",
        "print(\"\\nSample of extracted entities:\")\n",
        "print(processed_df[['text', 'persons', 'places', 'organisations']].head())\n",
        "\n",
        "print(\"\\nTop 5 liked tweets:\")\n",
        "print(analysis_results['top_likes'])\n",
        "\n",
        "print(\"\\nTop 5 retweeted tweets:\")\n",
        "print(analysis_results['top_retweets'])\n",
        "\n",
        "print(\"\\nMost common day for tweets:\", analysis_results['most_common_day'])\n",
        "\n",
        "print(\"\\nAnalysis complete. Check the following output files:\")\n",
        "print(\"1. processed_tweets.csv - Complete processed dataset\")\n",
        "print(\"2. tweet_links.txt - List of tweet URLs\")\n",
        "print(\"3. url_links.txt - List of URLs from tweets\")\n",
        "print(\"4. media_links.txt - List of media links\")\n",
        "print(\"5. top_likes.csv - Top liked tweets\")\n",
        "print(\"6. top_retweets.csv - Top retweeted tweets\")\n",
        "print(\"7. earliest_user_tweets.csv - Tweets from earliest user\")\n",
        "print(\"8. most_followed_tweets.csv - Tweets from most followed user\")\n",
        "print(\"9. verified_users.csv - Tweets from verified users\")\n",
        "print(\"10. tweets_per_day.png - Visualization of tweet frequency\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrqRyQcEDuF7",
        "outputId": "a62d1f5a-a49c-4672-c30d-79c638efacc7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data analysis...\n",
            "\n",
            "Part 1: Cleaning data...\n",
            "\n",
            "Part 2: Performing analysis...\n",
            "\n",
            "Part 3: Processing text with NLP...\n",
            "\n",
            "Part 4: Creating visualization...\n",
            "\n",
            "Results preview:\n",
            "\n",
            "Sample of extracted entities:\n",
            "                                                text                  persons  \\\n",
            "0  @beata_skwarska Warszawa üòÄ https://t.co/W7BcyS...                       []   \n",
            "1  Nieznani sprawcy podpalili kapliczkƒô nadrzewnƒÖ...                       []   \n",
            "2  ‚ö†Ô∏è Utrudnienia w komunikacji: L20 https://t.co...                       []   \n",
            "3  @LukaszKohut @moanrosa @LincaAgata @jan_jozef_...                       []   \n",
            "4  Dzieci to najwiƒôkszy skarb, o kt√≥ry musimy dba...  [#Dzie≈ÑDziecka, Franio]   \n",
            "\n",
            "                            places organisations  \n",
            "0      [@beata_skwarska, Warszawa]            []  \n",
            "1         [warszawskim, Grochowie]            []  \n",
            "2                               []            []  \n",
            "3  [Warszawa, polskiego, Warschau]            []  \n",
            "4                       [Warszawa]           [üëß]  \n",
            "\n",
            "Top 5 liked tweets:\n",
            "                                                   text  favorite_count\n",
            "4599  Dzi≈õ z prezydent-elekt @Tsihanouskaya m√≥wili≈õm...            1386\n",
            "4012  Warszawa czeka na @Tsihanouskaya. #StandWithBe...             744\n",
            "290   Jasio, kt√≥ry nie panuje nad swojƒÖ jadaczkƒÖ, zn...             623\n",
            "1466  Czyli prezydenci opozycji: \\nüëâPozna≈Ñ -cieszy s...             605\n",
            "1841  Pierwszy z 21 nowych pociƒÖg√≥w @SKM_Warszawa ju...             553\n",
            "\n",
            "Top 5 retweeted tweets:\n",
            "                                                   text  retweet_count\n",
            "622   HALO WARSZAWA\\nchƒôtnie rozja≈õniƒô, ufarbujƒô, ze...           1638\n",
            "4383  ‚ÄºÔ∏è\\nA teraz pere≈Çka. Fina≈Ç Was rozwali. \\n10-1...            896\n",
            "182   UWAGA #Warszawa ≈öniadeckich 12/16\\n\\nPani Danu...            865\n",
            "217   UWAGA #Warszawa ≈öniadeckich 12/16\\n\\nPani Danu...            865\n",
            "278   UWAGA #Warszawa ≈öniadeckich 12/16\\n\\nPani Danu...            865\n",
            "\n",
            "Most common day for tweets: Wed\n",
            "\n",
            "Analysis complete. Check the following output files:\n",
            "1. processed_tweets.csv - Complete processed dataset\n",
            "2. tweet_links.txt - List of tweet URLs\n",
            "3. url_links.txt - List of URLs from tweets\n",
            "4. media_links.txt - List of media links\n",
            "5. top_likes.csv - Top liked tweets\n",
            "6. top_retweets.csv - Top retweeted tweets\n",
            "7. earliest_user_tweets.csv - Tweets from earliest user\n",
            "8. most_followed_tweets.csv - Tweets from most followed user\n",
            "9. verified_users.csv - Tweets from verified users\n",
            "10. tweets_per_day.png - Visualization of tweet frequency\n"
          ]
        }
      ]
    }
  ]
}